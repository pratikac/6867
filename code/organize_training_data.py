"""
    This script uses the .jl JSON files generated by scraping FML etc. to
    generate consolidated corpora of positive and negative training data with
    scores post lemmatization
"""

import simplejson as json
from math import sqrt
from nltk.tokenize import RegexpTokenizer
from nltk.stem.wordnet import WordNetLemmatizer
import sys



# One sided confidence interval that the 'actual' score will be above the calcualted score

def calculate_score(upVotes, downVotes):
    nVotes = upVotes + downVotes
    if nVotes == 0:
        return 0

    confidence = 1.645        # use 1.0 for a confidence of 85%, 1.645 for 95%
    decimalFigures = 4   # report score to 4 decimal places

    P_hat = float(upVotes)/nVotes
    predicted_score =  (P_hat+confidence*confidence/(2*nVotes)-confidence*sqrt((P_hat*(1-P_hat)+confidence*confidence/(4*nVotes))/nVotes))/(1+confidence*confidence/nVotes)
    return round(predicted_score, decimalFigures)


def process_text(text):
    # Tokenize ONLY alphabetic sequences 
    wordTokenizer = RegexpTokenizer('[a-zA-Z]+')    # only pick out letters
    longWordTokenizer = RegexpTokenizer('[a-zA-Z]{3,}') #  > 2 letters 
    lmtzr = WordNetLemmatizer()

    # Lemmatize words in the word list 
    wordList = [lmtzr.lemmatize(word.lower()) for word in wordTokenizer.tokenize(text)]
    longWordList = [lmtzr.lemmatize(word.lower()) for word in longWordTokenizer.tokenize(text)]

    # Clean up after lemmatization - is there a more efficient way? 
    for wordNo, word in enumerate(wordList):
        # 'was' becomes 'wa' after lemmatization
        if word == 'wa':
            wordList[wordNo] = 'was'

    for wordNo, word in enumerate(longWordList):
        # 'was' becomes 'wa' after lemmatization
        if word == 'wa':
            longWordList[wordNo] = 'was'

    # Generate unigram  and bigram lists
    unigram_list = longWordList

    bigram_list = []
    for wordNo in range(len(wordList[:-1])):
        bigram_list.append(wordList[wordNo]+' '+wordList[wordNo+1])

    return (sorted(unigram_list), sorted(bigram_list))


def process_snippet(sourceName):
    positiveSources = ['LML', 'MLIG']
    negativeSources = ['FML']

    if sourceName in negativeSources:
        scoreSign = (-1.0)
        output_file = 'negative.jl'
    elif sourceName in positiveSources:
        scoreSign = (1.0)
        output_file = 'positive.jl'
    else:
        print "Snippet source not in list of sources!"
        return

    # Append snippets to output file
    with open(output_file, 'a') as outFile:
        scrapedFile = 'bitchySites/' + sourceName + '.jl' 
        print "Processing snippets from " + sourceName
        with open(scrapedFile, 'r') as jsonFile:
            for line in jsonFile:
                snippet = json.loads(line)

                # Snippet processing
                unigramList, bigramList = process_text(snippet['text'])
                # Assign a NEGATIVE score
                score = scoreSign * calculate_score(snippet['upVotes'], snippet['downVotes']) 
                snippetData = {'unigramList': unigramList, 'bigramList': bigramList, 'score': score, 'ID': sourceName+'_'+str(snippet['snippetID'])}
                outFile.write(json.dumps(snippetData)+"\n")

    print "Done!"


if __name__ == "__main__":
    for source_name in sys.argv[1:]:
        process_snippet(source_name)


