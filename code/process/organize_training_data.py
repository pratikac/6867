"""
    This script uses the .jl JSON files generated by scraping FML etc. to
    generate consolidated corpora of positive and negative training data with
    scores post lemmatization
"""

import json
from math import sqrt
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import RegexpTokenizer
from nltk.stem.wordnet import WordNetLemmatizer
import nltk
import sys
import pdb

sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
word_tokenizer = RegexpTokenizer('[a-zA-Z]{3,}')       # only pick out letters
lmtzr = WordNetLemmatizer()

# One sided confidence interval that the 'actual' score will be above the calcualted score

def calculate_score(upVotes, downVotes):
    nVotes = upVotes + downVotes
    if nVotes == 0:
        return 0

    confidence = 1.645        # use 1.0 for a confidence of 85%, 1.645 for 95%
    decimalFigures = 4   # report score to 4 decimal places

    P_hat = float(upVotes)/nVotes
    predicted_score =  (P_hat+confidence*confidence/(2*nVotes)-confidence*sqrt((P_hat*(1-P_hat)+confidence*confidence/(4*nVotes))/nVotes))/(1+confidence*confidence/nVotes)
    return round(predicted_score, decimalFigures)

'''
def process_text(text):
    # Tokenize ONLY alphabetic sequences 
    wordTokenizer = RegexpTokenizer('[a-zA-Z]+')    # only pick out letters
    longWordTokenizer = RegexpTokenizer('[a-zA-Z]{3,}') #  > 2 letters 
    lmtzr = WordNetLemmatizer()

    # 1. tokanize, remove stop words
    remove_stop_words = lambda x: if x not in stopwords.words('english')
    short_text = filter(remove_stop_words, wordTokenizer.tokenize(text))
    long_text = filter(remove_stop_words, longWordTokenizer.tokenize(text))

    # 2. lematize words
    wordList = [lmtzr.lemmatize(word.lower()) for word in wordTokenizer.tokenize(short_text)]
    longWordList = [lmtzr.lemmatize(word.lower()) for word in longWordTokenizer.tokenize(long_text)]
    
    # Generate unigram  and bigram lists
    unigram_list = longWordList

    bigram_list = []
    for wordNo in range(len(wordList[:-1])):
        bigram_list.append(wordList[wordNo]+' '+wordList[wordNo+1])

    return (sorted(unigram_list), sorted(bigram_list))
'''

def process_text(text):
    pos_tagger = nltk.pos_tag
    sentences = sent_tokenizer.tokenize(text)
    lemmatizer_format = {'NN':'n', 'JJ':'a','VBD':'v'}
    remove_stop_words = lambda x: (x not in stopwords.words('english')) and (x != 'fml') and (x!='lml') and (x!='mlig')
    keep_only_useful_pos = lambda x: x[1] in lemmatizer_format.keys()
    convert_to_lemmatize_format = lambda x: (x[0], lemmatizer_format[x[1]]) 
    word_list = set()
    for sent in sentences:
        words = word_tokenizer.tokenize(sent)
        words = filter(remove_stop_words, words)
        words_n_pos = filter(keep_only_useful_pos, pos_tagger(words))
        words_n_pos = map(convert_to_lemmatize_format, words_n_pos)
        word_list.update(set(map(lambda wp: lmtzr.lemmatize(wp[0].lower(), wp[1]), words_n_pos)))
  
    return (sorted(list(word_list)), [])

def process_snippet(sourceName):
    positiveSources = ['LML', 'MLIG']
    negativeSources = ['FML']

    if sourceName in negativeSources:
        scoreSign = (-1.0)
        output_file = 'negative.jl'
    elif sourceName in positiveSources:
        scoreSign = (1.0)
        output_file = 'positive.jl'
    else:
        print "Snippet source not in list of sources!"
        return

    # Append snippets to output file
    with open(output_file, 'a') as outFile:
        scrapedFile = '../crawler/' + sourceName + '.jl'
        print "Processing snippets from " + sourceName
        with open(scrapedFile, 'r') as jsonFile:
            for line in jsonFile:
                snippet = json.loads(line)

                # snippet processing
                unigramList, bigramList = process_text(snippet['text'])
                
                # Assign a NEGATIVE score
                score = scoreSign * calculate_score(snippet['upVotes'], snippet['downVotes']) 
                snippetData = {'unigramList': unigramList, 'bigramList': bigramList, 'score': score, 'ID': sourceName+'_'+str(snippet['snippetID'])}
                outFile.write(json.dumps(snippetData)+"\n")

    print "Done!"


if __name__ == "__main__":
    for source_name in sys.argv[1:]:
        process_snippet(source_name)

