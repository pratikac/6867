\documentclass[10pt, letterpaper, conference, final, twocolumn]{ieeeconf}
\IEEEoverridecommandlockouts
\overrideIEEEmargins

%\usepackage{mathpazo}
\input{macros.tex}

% margins
\setlength{\marginparwidth}{0.6in}
\definecolor{darkgreen}{rgb}{0,0.6,0} \newcommand{\dg}{\color{darkgreen}}
\definecolor{fullred}{rgb}{0.85,.0,.1} \newcommand{\fr}{\color{fullred}}
\definecolor{darkblue}{rgb}{0,0,1.0} \newcommand{\db}{\color{darkblue}}
\definecolor{brown}{rgb}{0.54,.27,0} \newcommand{\br}{\color{brown}}

\newcommand{\pcm}[2]{{\dg #1}\marginpar{\tiny\noindent{\raggedright{\dg[PC]}\br{ #2} \par}}}
\newcommand{\vsm}[2]{{\fr #1}\marginpar{\tiny\noindent{\raggedright{\dg[VS]}\db{ #2} \par}}}
%\renewcommand{\margin}[2]{#1}
\newcommand{\ignore}[1]{}

% save space
\newcommand{\algsize}{\footnotesize}
\setlength{\floatsep}{0.05in}
\setlength{\textfloatsep}{0.1in}
\setlength{\intextsep}{0.05in}
\setlength{\belowcaptionskip}{0.01in}
\setlength{\abovecaptionskip}{0.1in}
\setlength{\abovedisplayskip}{0.08in}
\setlength{\belowdisplayskip}{0.08in}

\begin{document}
\title{\bf Twitter-based Mood Evaluation
	\thanks{$^*$Chemical Engingeering Department, MIT. Email: \href{mailto:vishnusr@mit.edu}{vishnusr@mit.edu}}
	\thanks{$^\dag$Laboratory of Information and Decision Systems, MIT. Email: \href{mailto:pratikac@mit.edu}{pratikac@mit.edu}}
}
\author{Vishnu Sresht$^*$ \qquad Pratik Chaudhari$^\dag$}
\maketitle

\begin{abstract}
By providing a convenient, readily-accessible way to reach out to a global audience, twitter has revolutionized the way we broadcast our feelings to the rest of the world. In addition to its empowerment of the diva within us all, Twitter's meteoric rise in popularity has enabled every one of us to listen to the voices of millions and comprehend mankind's \textit{zeitgest} at an uprecedented resolution. However, the deluge of verbiage unleashed by Twitter's ubiquitous usage must be tamed before its wealth of information can be exploited for sociologically-beneficial research. In this project, we attempt to survey, compare and contrast machine learning techniques for one particular form of large-scale tweet analysis - that of determining `how positive people feel' at any given moment through the sentiment analysis of their tweets. To this end, we also introduce a novel source of already classified text corpora for use as training data. Sites like \href{http://mylifeisg.com}{My Life Is G} and \href{http://fmylife.com}{FML} are a hiherto unutilized source of crowd-curated, well classified training data of text snippets of appropriate length. In this paper, we intend to study the relative efficacies of several commonly employed classification algorithms, including Naive Bayes, Support Vector Machines, and K-Nearest Neighbor classifiers when applied to the task of of detecting the mood of the nation on a real-time basis.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\section{Scraping data}
\label{sec:data}

\section{Feature representation}
\label{sec:features}

various subsections for different methods.
\subsection{TF-IDF}
\label{ssec:tfidf}

\section{Preliminary results}
\label{sec:prelim}

\section{Latent topics}
\label{sec:latent}

The previous section demonstrated the performance of various classifers on the feature representations obtained from the data. This section builds upon the idea that words in a document are results of a few latent topics. In other words, rather than training a classifer to distinguish between words, we train it to distinguish between latent topics.

This idea has been pursued in literature in a number of different ways...\ldots.

Suppose that for the two classes, $+$ (documents that express positive sentiment) and $-$ (documents that express a negative sentiment), we have a set of latent topics. Let $p = \{ p_1, \ldots, p_m \}$ be the topics that result in words associated with $+$ and $q = \{ q_1, \ldots, q_n \}$ be the topics that result in words associated with $-$. Each sentiment $+$, $-$ is a noisy observation based upon these latent topics. However, we do not know what these topics are and hence we create a simple model based on them.

Denote the words in a document by $s$. Let the probability of generating an observation $s$ from latent topics $p$ be
$$
\PP(s | p) = 
$$

\section{Conclusions}
\label{sec:conclusions}

\bibliography{writeup}
\bibliographystyle{unsrt}

\end{document}

